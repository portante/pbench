#!/usr/bin/env python3

# Example curl command sequence
#
#   $ md5sum tool-data.tar.xz > tool-data.tar.xz.md5
#   $ curl -X PUT -H "MD5SUM: $(awk '{print $1}' tool-data.tar.xz.md5)" \
#     http://localhost:8080/tool-data/XXX...XXX/perf48.example.com \
#     --data-binary @tool-data.tar.xz

# Needs daemon, pidfile, and bottle
#   sudo dnf install python3-bottle python3-daemon
#   sudo pip3 install python-pidfile

import sys
import os
import socket
import subprocess
import shutil
import tarfile
import json
import hashlib
import tempfile
import logging
import daemon
import pidfile
import redis
import errno
from pathlib import Path
from datetime import datetime
from threading import Thread, Lock, Condition
from bottle import Bottle, ServerAdapter, request, abort
from wsgiref.simple_server import make_server, WSGIRequestHandler

_BUFFER_SIZE = 65536



class QuietHandler(WSGIRequestHandler):
    """QuietHandler - a simple class to ignore request logging.
    """

    def log_request(*args, **kw):
        pass


class DataSinkWSGIRefServer(ServerAdapter):
    """A instance of a WSGI "simple server" with the option to "quiet" the request
    logging.

    Taken from https://stackoverflow.com/questions/11282218/bottle-web-framework-how-to-stop.
    """

    def __init__(self, *args, quiet=False, **kw):
        super().__init__(*args, **kw)
        if quiet:
            self.options["handler_class"] = QuietHandler
        self._server = None

    def run(self, handler):
        assert self._server is None, f"'run' method called twice"
        self._server = make_server(self.host, self.port, handler, **self.options)
        self._server.serve_forever()

    def stop(self):
        if self._server is not None:
            self._server.shutdown()


class ToolDataSink(Bottle):
    """ToolDataSink - sub-class of Bottle representing state for tracking data
    sent from tool meisters via an HTTP PUT method.
    """

    class Terminate(Exception):
        pass

    def __init__(self, redis_server, channel, benchmark_run_dir, logger):
        super(ToolDataSink, self).__init__()
        # Save external state
        self.redis_server = redis_server
        self.channel = channel
        self.benchmark_run_dir = benchmark_run_dir
        self.logger = logger
        # Initialize internal state
        self._hostname = os.environ["full_hostname"]
        self.state = None
        self.tool_data_ctx = None
        self.directory = None
        self._data = None
        self._tm_tracking = None
        self._put_count = 0
        self._lock = Lock()
        self._cv = Condition(lock=self._lock)
        # Setup the Bottle server route and the WSGI server instance.
        self.route(
            "/tool-data/<tool_data_ctx>/<hostname>",
            method="PUT",
            callback=self.put_document,
        )

        self._server = DataSinkWSGIRefServer(host="localhost", port=8080)
        # Setup the Redis server channel subscription
        self._pubsub = redis_server.pubsub()
        self._pubsub.subscribe(channel)
        self._chan = self._pubsub.listen()
        # Pull off first message which is an acknowledgement we have
        # successfully subscribed.
        resp = next(self._chan)
        assert resp["type"] == "subscribe", f"bad type: {resp!r}"
        assert resp["pattern"] is None, f"bad pattern: {resp!r}"
        assert resp["channel"].decode("utf-8") == channel, f"bad channel: {resp!r}"
        assert resp["data"] == 1, f"bad data: {resp!r}"
        # Tell the entity that started us who we are indicating we're ready.
        started_msg = dict(kind="ds", hostname=self._hostname, pid=os.getpid())
        redis_server.publish(f"{channel}-start", json.dumps(started_msg))

    def run(self):
        """run - Start the Bottle web server running and the watcher thread.
        """
        watcher_thread = Thread(target=self.watcher)
        watcher_thread.start()

        self.logger.info("Running...")
        super().run(server=self._server)

        self.logger.debug("Waiting for the watcher thread to exit ...")
        watcher_thread.join()

    def _cleanup(self):
        """_cleanup - Encapsulates the proper shutdown sequence for the WSGI server
        and Redis Server connection.
        """
        try:
            self._pubsub.unsubscribe()
            self._pubsub.close()
        except Exception:
            self.logger.exception(
                "failed to properly close the Redis server connection state"
            )
        self._server.stop()

    def watcher(self):
        """Simple function for the thread that is "watching" for the terminate
        state.
        """
        self.logger.info("watcher started")
        try:
            for payload in self._chan:
                try:
                    json_str = payload["data"].decode("utf-8")
                except Exception:
                    self.logger.warning(
                        "data payload in message not UTF-8, '%r'", json_str
                    )
                    continue
                self.logger.debug('watcher: channel payload, "%r"', json_str)
                try:
                    data = json.loads(json_str)
                except json.JSONDecodeError:
                    self.logger.warning(
                        "data payload in message not JSON, '%s'", json_str
                    )
                    continue
                else:
                    try:
                        data["state"]
                    except KeyError:
                        self.logger.warning(
                            "unrecognized data payload in message," " '%s'", json_str
                        )
                    else:
                        self.state_change(data)
        except self.Terminate as exc:
            self.logger.info("%s", exc)
        except redis.exceptions.ConnectionError:
            self.logger.warning(
                "watcher closing down after losing connection to redis server"
            )
        except Exception:
            self.logger.exception("watcher exception")
        finally:
            self._cleanup()

    def _fetch_tms(self):
        """_fetch_tms - fetch all the Tool Meister data for all recorded tool
        meisters in the Redis server under the tm-pids key.

        We return to the caller a dictionary indexed by the host name of each
        tool meister found.

        NOTE: this method should only be called when we know the "tm-pids" key
        is properly populated.
        """
        tms = {}
        pids_json_str_raw = self.redis_server.get("tm-pids")
        if pids_json_str_raw is None:
            raise Exception("Missing 'tm-pids' data on Redis server")
        self.logger.debug("tm-pids: %r", pids_json_str_raw)
        try:
            pids_json_str = pids_json_str_raw.decode("utf-8")
            pids = json.loads(pids_json_str)
        except Exception as exc:
            raise Exception(f"failed parse 'tm-pids' JSON payload, '{exc}'")
        else:
            # Double check our data sink entry is as expected.
            assert pids["ds"]["kind"] == "ds", f"what? {pids['ds']!r}"
            assert pids["ds"]["pid"] == os.getpid(), f"what? {pids['ds']!r}"
            assert pids["ds"]["hostname"] == self._hostname, f"what? {pids['ds']!r}"
            for tm in pids["tm"]:
                assert tm["kind"] == "tm", f"what? {tm!r}"
                if tm["hostname"] == self._hostname:
                    # The "localhost" tool meister instance does not send data
                    # to the tool data sink, it just writes it locally.
                    continue
                # Posted is None to start, "yes" when we receive data for Tool
                # Mesiter host, "waiting" when in the "sending" state, and
                # "dormant" in other states indicating we should not be
                # receiving any data at that time.
                tm["posted"] = None
                tms[tm["hostname"]] = tm
        return tms

    def _wait_for_all_data(self):
        """wait_for_all_data - block the caller until all of the registered
        tool meisters have sent their data.

        Waiting is a no-op for all states except 'send'.  In the 'send' state,
        we are expecting to hear from all registered tool meisters.
        """

        if self.state != "send":
            return

        with self._lock:
            assert (
                self._tm_tracking is not None
            ), f"Logic bomb! self._tm_tracking is None"
            done = False
            while not done:
                for hostname, tm in self._tm_tracking.items():
                    if tm["posted"] is None or tm["posted"] == "dormant":
                        done = True
                        # Don't bother checking any other Tool Meister.  If we
                        # have one that has not been setup or is dormant, we
                        # don't need to wait at all.
                        break
                    if tm["posted"] == "waiting":
                        # Don't bother checking any other Tool Meister when we
                        # have at least one that has not sent any data.
                        break
                    assert tm["posted"] == "yes", f"Logic bomb! {tm['posted']!r}"
                else:
                    # We have checked every Tool Meister tracking record and
                    # they all have posted.  So we can safely exit the wait
                    # loop
                    done = True
                if not done:
                    while self._put_count == 0:
                        self._cv.wait()
        return

    def _change_tm_tracking(self, curr, new):
        """_change_tm_tracking - if we have a tool meister tracking dictionary
        update the posted state from the current expected value to the target
        new value.

        No changes take place if the tool meister tracking dictionary does not
        exist yet.

        Assumes self._lock is already acquired by our caller.
        """
        if self._tm_tracking is None:
            return
        for hostname, tm in self._tm_tracking.items():
            assert (
                tm["posted"] is None or tm["posted"] == curr
            ), f"_change_tm_tracking unexpected tm posted value, {tm!r}"
            tm["posted"] = new

    def state_change(self, data):
        """state_change - give a data dictionary, change the state for this
        data sink instance.

        The "watcher" thread has already validated the state field, we then
        validate the directory field.

        Public method, returns None, raises no exceptions explicitly, called
        by the "watcher" thread.
        """
        # We have a valid state change data set, wait for all the tool
        # meisters before proceeding.
        self._wait_for_all_data()


        if self.state is None:
            # This is the first published state change we have received.  We
            with self._lock:
                assert self._tm_tracking is None, (
                    f"Logic bomb! self._tm_tracking '{self._tm_tracking!r}'"
                    " is not None"
                )
                # Typically, we only call this method once when the Tool Data
                # Sink first starts since the list of Tool Meister's is
                # static.  We call it when we get the first message to
                # be sure the "tm-pids" key exists.
                #
                # FIXME: what happens when a Tool Meister dies, and rejoins?
                self._tm_tracking = self._fetch_tms()

        self._data = data
        self.state = data["state"]
        if self.state == "terminate":
            raise self.Terminate("Terminate bottle server")
        directory_str = data["directory"]
        directory = Path(directory_str)
        if not directory.is_dir():
            self.logger.error(
                "state change to '{}' with non-existent directory, '{}'",
                data["state"],
                directory,
            )
        self.directory = directory
        # The tool meisters will be hashing the directory argument this way
        # when invoking the PUT method.  They just consider the directory
        # argument to be an opaque context. We, the tool data sink, write the
        # data we receive to that directory.
        directory_bytes = directory_str.encode("utf-8")
        self.tool_data_ctx = hashlib.md5(directory_bytes).hexdigest()

        # Transition to "send" state should reset self._tm_tracking
        with self._lock:
            # We should never see a positive PUT count as all data should have
            # been received by now.
            assert (
                self._put_count == 0
            ), f"Logic bomb! self._put_count '{self._put_count}' != 0"
            if self.state == "send":
                self._change_tm_tracking("dormant", "waiting")
            elif self.state == "start":
                self._change_tm_tracking("yes", "dormant")
            else:
                assert self.state == "stop", f"Unexpected state, '{self.state}'"
                # Nothing to do, no data movement for "stop"

    def put_document(self, tool_data_ctx, hostname):
        """put_document - PUT callback method for Bottle web server end point

        The put_document method is called by threads serving web requests.
        There can be N threads configured at one time calling this method.

        Public method, returns None, raises no exceptions directly, calls the
        Bottle abort() method for error handling.

        """
        if self.state != "send":
            # FIXME: Don't we have a race condition if the tool meisters
            # process their messages first?  Seems like we need to send to the
            # Tool Data Sink first, and then send to all the tool meisters.
            abort(400, f"Can't accept PUT requests in state '{self.state}'")

        if self.tool_data_ctx != tool_data_ctx:
            abort(400, f"Unexpected tool data context, '{tool_data_ctx}'")

        try:
            content_length = int(request["CONTENT_LENGTH"])
        except ValueError:
            abort(400, "Invalid content-length header, not an integer")
        except Exception:
            abort(400, "Missing required content-length header")
        else:
            if content_length > (2 ** 30):
                abort(
                    400,
                    "Content object too large, keep it at 1 GB"
                    f" ({content_length:d}) and  under",
                )
            remaining_bytes = content_length

        try:
            exp_md5 = request["HTTP_MD5SUM"]
        except Exception:
            self.logger.exception(request.keys())
            abort(400, "Missing required md5sum header")

        target_dir = self.benchmark_run_dir / self.directory
        if not target_dir.is_dir():
            self.logger.error("ERROR - directory, '%s', does not exist", target_dir)
            abort(500, f"Invalid URL, path {target_dir} does not exist")
        host_tool_data_tb_name = target_dir / f"{hostname}.tar.xz"
        if host_tool_data_tb_name.exists():
            abort(409, f"{host_tool_data_tb_name} already uploaded")
        host_tool_data_tb_md5 = Path(f"{host_tool_data_tb_name}.md5")

        with tempfile.NamedTemporaryFile(mode="wb", dir=target_dir) as ofp:
            total_bytes = 0
            iostr = request["wsgi.input"]
            h = hashlib.md5()
            while remaining_bytes > 0:
                buf = iostr.read(
                    _BUFFER_SIZE if remaining_bytes > _BUFFER_SIZE else remaining_bytes
                )
                bytes_read = len(buf)
                total_bytes += bytes_read
                remaining_bytes -= bytes_read
                h.update(buf)
                ofp.write(buf)
            cur_md5 = h.hexdigest()
            if cur_md5 != exp_md5:
                abort(
                    400,
                    f"Content, {cur_md5}, does not match its MD5SUM header,"
                    f" {exp_md5}",
                )
            if total_bytes <= 0:
                abort(400, "No data received")

            # First write the .md5
            try:
                with host_tool_data_tb_md5.open("w") as md5fp:
                    md5fp.write(f"{exp_md5} {host_tool_data_tb_name.name}\n")
            except Exception:
                try:
                    os.remove(host_tool_data_tb_md5)
                except Exception:
                    self.logger.warning(
                        "Failed to remove .md5 %s when trying to clean" " up",
                        host_tool_data_tb_md5,
                    )
                raise

            # Then create the final filename link to the temporary file.
            os.link(ofp.name, host_tool_data_tb_name)
            self.logger.info(
                "Successfully wrote %s (%s.md5)",
                host_tool_data_tb_name,
                host_tool_data_tb_name,
            )
            # Tell the waiting "watcher" thread engaging in a "state" change
            # that another PUT document has arrived.
            with self._lock:
                tm_tracker = self._tm_tracking[hostname]
                assert tm_tracker["posted"] == "waiting", f"tm_tracker = {tm_tracker!r}"
                tm_tracker["posted"] = "yes"
                self._put_count += 1
                self._cv.notify()

def prometheus_launch():
    config = open("prometheus.yml", 'w')

    config.write("global:\n  scrape_interval: 1s\n  evaluation_interval: 1s\n\n")
    #config.write("alerting:\n  alertmanagers:\n  - static_configs:\n    - targets:\n\nrule_files:\n\n")
    config.write("scrape_configs:\n  - job_name: \'prometheus\'\n    static_configs:\n    - targets: [\'localhost:9090\']\n\n")

    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    s.connect(("8.8.8.8", 80))
    ip_address = s.getsockname()[0]
    s.close()

    for i in range(1):
        config.write("  - job_name: \'local_node\'\n    static_configs:\n    - targets: [\'{}:9100\']\n\n".format(str(ip_address)))

    config.close()

    image = open("Dockerfile", 'w')
    image.write("FROM prom/prometheus\nADD prometheus.yml /etc/prometheus/")
    image.close()

    args = ['podman', 'pull', 'prom/prometheus']
    prom_pull = subprocess.Popen(args, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)
    prom_pull.wait()

    args = ['podman', 'build', '-t', 'custom-prom', '-f', './Dockerfile']
    build = subprocess.Popen(args, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)
    build.wait()

    now = datetime.now()
    volume = "prom_data_" + now.strftime("%d-%m-%Y_%Hh%Mm%Ss")

    args = ['podman', 'run', '-p', '9090:9090', '-v', '{}:/prometheus'.format(volume), 'custom-prom']
    run = subprocess.Popen(args, stdout =subprocess.DEVNULL, stderr=subprocess.STDOUT)

    return (run, volume)

def main(argv):
    PROG = Path(argv[0]).name

    logger = logging.getLogger(PROG)
    fh = logging.FileHandler(f"{PROG}.log")
    if os.environ.get("_PBENCH_UNIT_TESTS"):
        fmtstr = "%(levelname)s %(name)s %(funcName)s -- %(message)s"
    else:
        fmtstr = (
            "%(asctime)s %(levelname)s %(process)s %(thread)s"
            " %(name)s %(funcName)s %(lineno)d -- %(message)s"
        )
    fhf = logging.Formatter(fmtstr)
    fh.setFormatter(fhf)
    fh.setLevel(logging.INFO)
    logger.addHandler(fh)
    logger.setLevel(logging.INFO)

    try:
        redis_host = argv[1]
        redis_port = argv[2]
        param_key = argv[3]
    except IndexError as e:
        logger.error("Invalid arguments: %s", e)
        return 1

    try:
        redis_server = redis.Redis(host=redis_host, port=redis_port, db=0)
    except Exception as e:
        logger.error(
            "Unable to connect to redis server, %s:%s: %s", redis_host, redis_port, e
        )
        return 2

    try:
        params_raw = redis_server.get(param_key)
        if params_raw is None:
            logger.error('Parameter key, "%s" does not exist.', param_key)
            return 3
        logger.info("params_key (%s): %r", param_key, params_raw)
        params_str = params_raw.decode("utf-8")
        # The expected parameters for this "data-sink" is what "channel" to
        # subscribe to for the tool meister operational life-cycle.  The
        # data-sink listens for the state transitions, start | stop | send |
        # terminate, exiting when "terminate" is received, marking the state
        # in which data is captured.
        #
        # E.g. params = '{ "channel": "run-chan",
        #                  "benchmark_run_dir": "/loo/goo" }'
        params = json.loads(params_str)
        channel = params["channel"]
        benchmark_run_dir = Path(params["benchmark_run_dir"]).resolve()
        tool_group = params["group"]
        #logger.info("TOOL GROUP: " + str(tool_group))
    except Exception as ex:
        logger.error("Unable to fetch and decode parameter key, %s: %s", param_key, ex)
        return 4
    else:
        if not benchmark_run_dir.is_dir():
            logger.error(
                "Run directory argument, %s, must be a real" " directory.",
                benchmark_run_dir,
            )
            return 5

    #run = None
    run,volume = prometheus_launch()

    pidfile_name = f"{PROG}.pid"
    pfctx = pidfile.PIDFile(pidfile_name)
    with open(f"{PROG}.out", "w") as sofp, open(f"{PROG}.err", "w") as sefp:
        with daemon.DaemonContext(
            stdout=sofp,
            stderr=sefp,
            working_directory=os.getcwd(),
            umask=0o022,
            pidfile=pfctx,
            files_preserve=[sofp.fileno(), sefp.fileno(), fh.stream.fileno()],
        ):
            try:
                # We have to re-open the connection to the redis server now that we
                # are "daemonized".
                try:
                    redis_server = redis.Redis(host=redis_host, port=redis_port, db=0)

                except Exception as e:
                    logger.error(
                        "Unable to connect to redis server, %s:%s: %s",
                        redis_host,
                        redis_port,
                        e,
                    )
                    return 2

                tds_app = ToolDataSink(redis_server, channel, benchmark_run_dir, logger)
                tds_app.run()
            except OSError as exc:
                if exc.errno == errno.EADDRINUSE:
                    logger.error(
                        "ERROR - tool data sink failed to start, localhost:8080 already in use"
                    )
                else:
                    logger.exception("ERROR - failed to start the tool data sink")
            except Exception:
                logger.exception("ERROR - failed to start the tool data sink")
            finally:
                logger.info("Remove pid file ... (%s)", pidfile_name)
                try:
                    os.unlink(pidfile_name)
                except Exception:
                    logger.exception("Failed to remove pid file %s", pidfile_name)

    run.terminate()
    run.wait()

    logger.info("BENCHMARK RUN DIR: " + str(benchmark_run_dir))

    data_tar = tarfile.open(str(benchmark_run_dir) + "/prometheus_data.tar.gz", "w:gz")
    source = "/var/lib/containers/storage/volumes/{}".format(volume)
    data_tar.add(source, arcname=os.path.basename(source))
    data_tar.close()

    #shutil.copytree("/var/lib/containers/storage/volumes/{}/".format(volume), os.getcwd() + "/{}".format(volume))

    shutil.rmtree("/var/lib/containers/storage/volumes/{}".format(volume))

    #os.remove("prometheus.yml")
    #os.remove("Dockerfile")

    return 0


if __name__ == "__main__":
    status = main(sys.argv)
    sys.exit(status)
